CREATE EXTENSION IF NOT EXISTS pg_trgm;

CREATE TABLE ptai.com_resultado_tx_completo (
    nr_trabalho SERIAL PRIMARY KEY,
    obj_trab TEXT,
    justif TEXT,
    tx_rel_atvd TEXT,
    tx_constatacao TEXT,
    combined_text TEXT,
    embedding_vector vector(300)
);

CREATE INDEX com_resultado_tx_completo_vec_idx
  ON ptai.com_resultado_tx_completo
  USING ivfflat (embedding_vector vector_cosine_ops)
  WITH (lists = 100);

CREATE INDEX com_resultado_tx_completo_vec_idx
  ON ptai.com_resultado_tx_completo
  USING ivfflat (embedding_vector vector_cosine_ops)
  WITH (lists = 100);

# Necessary imports
import psycopg2
from psycopg2.extras import execute_values
import pandas as pd
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity, linear_kernel
from string import punctuation
from scipy.sparse import csr_matrix
import unicodedata
import re
import numpy as np
import os
import sys
import unicodedata
import re

from spacy.lang.pt.stop_words import STOP_WORDS
from string import punctuation

# Load the Portuguese language model
nlp = spacy.load('pt_core_news_lg', exclude=["tok2vec", "tagger", "parser", "ner", "attribute_ruler"])

# Define stop words and unwanted words
STOP_WORDS = set(nlp.Defaults.stop_words)
unwanted_words = ['processo', 'cliente', 'estratégia', 'Avaliar', 'Responsabilidades', 'gestor', 'MRLD', 'risco', 'específico', 'tomador', 'Risco']
STOP_WORDS -= set(unwanted_words)

# Caminho para o diretório onde o módulo conexoes.py está localizado
caminho_para_conexoes = '/dados/notebooksServidor/conexoes'

# Adicionando o caminho ao sys.path
if caminho_para_conexoes not in sys.path:
    sys.path.append(caminho_para_conexoes)
    
from conexoes import *

conn, engine = postgres()
connV, engineV = postgresVector()

query = """
SELECT nr_trabalho, obj_trab, justif, tx_rel_atvd, tx_constatacao
       FROM ptai.com_resultado_tx_completo limit 10;
"""
df = pd.read_sql(query, engine)

#df['combined_text'] = df.apply(lambda row: f"[trabalho]: {row['nr_trabalho']}, [objetivo trabalho]: {row['obj_trab']}, [justificativa trabalho]: {row['justif']}, [relatorio auditoria]: {row['tx_rel_atvd']}, [principal constatação no relatorio]: {row['tx_constatacao']}", axis=1)

df['combined_text'] = df.apply(lambda row: f"[objetivo trabalho]: {row['obj_trab']}", axis=1)

#df.to_sql('com_resultado_tx_completo', engineV, schema='ptai', if_exists='append', index=False)

def normalize_text(text):
    # Converte para minúsculas
    text = text.lower()

    # Remove acentos (unicode -> ASCII)
    text = unicodedata.normalize('NFD', text)
    text = text.encode('ascii', 'ignore').decode('utf-8')

    # Remove pontuação e caracteres especiais (opcional)
    text = re.sub(r'[^\w\s]', '', text)

    # Remove espaços extras
    text = text.strip()

    return text

def tokenize_text_pt(text):
    text = text.lower()
    # Remove acentos
    text = unicodedata.normalize('NFD', text).encode('ascii','ignore').decode('utf-8')
    # Remove pontuação
    text = re.sub(r'[^\w\s]', '', text)
    # Split em espaços
    tokens = text.split()
    # Filtra stopwords e palavras vazias
    tokens = [t for t in tokens if t and t not in STOP_WORDS]
    return tokens

def update_embeddings(df, connV):
    """
    Exemplo: atualiza embedding_vector E tokens
    """
    with connV.cursor() as curV:
        for index, row in df.iterrows():
            nr_trabalho = row['nr_trabalho']
            combined_text = row['combined_text']

            # 1) Gera embedding do texto
            doc = nlp(combined_text)
            embedding_spacy = doc.vector.tolist()

            # 2) Gera tokens
            tokens = tokenize_text_pt(combined_text)

            # 3) Faz UPDATE com embedding e tokens
            curV.execute("""
                UPDATE ptai.com_resultado_tx_completo
                SET embedding_vector = %s,
                    tokens = %s
                WHERE nr_trabalho = %s
            """, (embedding_spacy, tokens, nr_trabalho))

    connV.commit()

update_embeddings(df, connV)

def search_similar_texts(query, connV, top_k=5):
    """
    Busca semântica + trigram, e também retorna:
    - palavras_encontradas (array)
    - qtd_encontradas (int)
    num DataFrame final.
    """
    # 1) Tokeniza a query do usuário
    query_tokens = tokenize_text_pt(query)

    # 2) Gera embedding da query (para parte vetorial)
    doc_query = nlp(query)
    embedding_query = doc_query.vector.tolist()
    embedding_query_str = f"ARRAY{embedding_query}"

    # 3) Puxa do banco: nr_trabalho, combined_text, tokens, distance, trigram
    with connV.cursor() as curV:
        curV.execute(f"""
            SELECT
                nr_trabalho,
                combined_text,
                tokens,
                embedding_vector <-> {embedding_query_str}::vector AS vector_distance,
                similarity(combined_text, %s) AS trigram_similarity
            FROM ptai.com_resultado_tx_completo
            ORDER BY embedding_vector <-> {embedding_query_str}::vector
            LIMIT {top_k * 5}
        """, (query,))

        rows = curV.fetchall()

    # rows = [(nr_trab, text, tokens, dist, trigram_sim), ...]

    # 4) Monta lista + DataFrame
    resultados_com_score = []
    for row in rows:
        nr_trabalho, combined_text, tokens_db, vector_distance, trigram_similarity = row

        # Normaliza a distância => similaridade vetorial ~ [0..1]
        if vector_distance is not None:
            sim_vector = 1 / (1 + vector_distance)
        else:
            sim_vector = 0

        # Combina o valor de trigram (só se não for None)
        trigram_sim = trigram_similarity if trigram_similarity else 0

        # Score final (exemplo: média)
        final_score = (sim_vector + trigram_sim) / 2

        # 5) Interseção: quais tokens do query estão em tokens_db?
        #    tokens_db é list ou None. Verifique:
        palavras_encontradas = []
        if tokens_db:
            set_tokens_db = set(tokens_db)
            set_query = set(query_tokens)
            # Interseção
            found = set_tokens_db.intersection(set_query)
            palavras_encontradas = list(found)
        
        qtd_encontradas = len(palavras_encontradas)

        resultados_com_score.append((
            nr_trabalho,
            combined_text,
            vector_distance,
            trigram_sim,
            final_score,
            palavras_encontradas,
            qtd_encontradas
        ))

    # 6) Converter em DataFrame
    df_result = pd.DataFrame(
        resultados_com_score,
        columns=[
            'nr_trabalho',
            'combined_text',
            'vector_distance',
            'trigram_similarity',
            'final_score',
            'palavras_encontradas',
            'qtd_encontradas'
        ]
    )

    # 7) Ordenar desc por final_score
    df_result.sort_values('final_score', ascending=False, inplace=True)

    # 8) Retornar top_k
    return df_result.head(top_k)

consulta = '''
fat rade codefat atos administrativos e demostrações
'''

df_result = search_similar_texts(consulta, connV, top_k=5)
