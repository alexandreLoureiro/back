from pgvector.psycopg import register_vector


CREATE EXTENSION IF NOT EXISTS pg_trgm;

CREATE TABLE ptai.com_resultado_tx_completo (
    nr_trabalho SERIAL PRIMARY KEY,
    obj_trab TEXT,
    justif TEXT,
    tx_rel_atvd TEXT,
    tx_constatacao TEXT,
    combined_text TEXT,
    embedding_vector vector(300)
);

CREATE INDEX com_resultado_tx_completo_vec_idx
  ON ptai.com_resultado_tx_completo
  USING ivfflat (embedding_vector vector_cosine_ops)
  WITH (lists = 100);

CREATE INDEX com_resultado_tx_completo_vec_idx
  ON ptai.com_resultado_tx_completo
  USING ivfflat (embedding_vector vector_cosine_ops)
  WITH (lists = 100);

import psycopg2
from psycopg2.extras import execute_values
import pandas as pd
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity, linear_kernel
from string import punctuation
from scipy.sparse import csr_matrix
import unicodedata
import re
import numpy as np
import os
import sys
import unicodedata
import re


# In[2]:


from spacy.lang.pt.stop_words import STOP_WORDS
from string import punctuation


# In[3]:


# Load the Portuguese language model
nlp = spacy.load('pt_core_news_lg', exclude=["tok2vec", "tagger", "parser", "ner", "attribute_ruler"])

# Define stop words and unwanted words
STOP_WORDS = set(nlp.Defaults.stop_words)
unwanted_words = ['processo', 'cliente', 'estratégia', 'Avaliar', 'Responsabilidades', 'gestor', 'MRLD', 'risco', 'específico', 'tomador', 'Risco']
STOP_WORDS -= set(unwanted_words)


# In[4]:


# Caminho para o diretório onde o módulo conexoes.py está localizado
caminho_para_conexoes = '/dados/notebooksServidor/conexoes'

# Adicionando o caminho ao sys.path
if caminho_para_conexoes not in sys.path:
    sys.path.append(caminho_para_conexoes)
    
from conexoes import *


# In[54]:


conn, engine = postgres()
connV, engineV = postgresVector()


# In[6]:


query = """
SELECT nr_trabalho, obj_trab, justif, tx_rel_atvd, tx_constatacao
       FROM ptai.com_resultado_tx_completo;
"""
df = pd.read_sql(query, engine)


# In[7]:


df.shape


# In[34]:


df['combined_text'] = df.apply(lambda row: f"[trabalho]: {row['nr_trabalho']}, [objetivo trabalho]: {row['obj_trab']}, [justificativa trabalho]: {row['justif']}, [relatorio auditoria]: {row['tx_rel_atvd']}, [principal constatação no relatorio]: {row['tx_constatacao']}", axis=1)


# In[35]:


#df['combined_text'] = df.apply(lambda row: f"[objetivo trabalho]: {row['obj_trab']}", axis=1)


# In[36]:


df.columns


# In[37]:


df.to_sql('com_resultado_tx_completo', engineV, schema='ptai', if_exists='append', index=False)


# In[38]:


def normalize_text(text):
    # Converte para minúsculas
    text = text.lower()

    # Remove acentos (unicode -> ASCII)
    text = unicodedata.normalize('NFD', text)
    text = text.encode('ascii', 'ignore').decode('utf-8')

    # Remove pontuação e caracteres especiais (opcional)
    text = re.sub(r'[^\w\s]', '', text)

    # Remove espaços extras
    text = text.strip()

    return text


# In[39]:


def tokenize_text_pt(text):
    text = text.lower()
    # Remove acentos
    text = unicodedata.normalize('NFD', text).encode('ascii','ignore').decode('utf-8')
    # Remove pontuação
    text = re.sub(r'[^\w\s]', '', text)
    # Split em espaços
    tokens = text.split()
    # Filtra stopwords e palavras vazias
    tokens = [t for t in tokens if t and t not in STOP_WORDS]
    return tokens


# In[40]:


def update_embeddings(df, connV):
    """
    Exemplo: atualiza embedding_vector E tokens
    """
    with connV.cursor() as curV:
        for index, row in df.iterrows():
            nr_trabalho = row['nr_trabalho']
            combined_text = row['combined_text']

            # 1) Gera embedding do texto
            doc = nlp(combined_text)
            embedding_spacy = doc.vector.tolist()

            # 2) Gera tokens
            tokens = tokenize_text_pt(combined_text)

            # 3) Faz UPDATE com embedding e tokens
            curV.execute("""
                UPDATE ptai.com_resultado_tx_completo
                SET embedding_vector = %s,
                    tokens = %s
                WHERE nr_trabalho = %s
            """, (embedding_spacy, tokens, nr_trabalho))

    connV.commit()


# In[41]:


update_embeddings(df, connV)


# In[75]:


def search_similar_texts(query, connV, top_k=5, select_query=None):
    try:
        # 1) Tokeniza a query do usuário
        query_tokens = tokenize_text_pt(query)
        set_query_tokens = set(query_tokens)

        # 2) Gera embedding da query (para parte vetorial)
        doc_query = nlp(query)
        embedding_query = doc_query.vector.tolist()
        embedding_query_str = f"ARRAY{embedding_query}"

        # 3) Puxa do banco
        if select_query is None:
            select_query = """
                SELECT
                    id,
                    combined_text,
                    tokens,
                    embedding_vector <-> {embedding_query_str}::vector AS vector_distance,
                    similarity(combined_text, %s) AS trigram_similarity
                FROM ptai.com_resultado_tx_completo
                ORDER BY embedding_vector <-> {embedding_query_str}::vector
                LIMIT {limit}
            """

        limit = top_k * 5
        select_query = select_query.format(embedding_query_str=embedding_query_str, limit=limit)

        with connV.cursor() as curV:
            curV.execute(select_query, (query,))
            rows = curV.fetchall()

        # Processa os resultados como antes
        resultados_com_score = []
        for row in rows:
            id, combined_text, tokens_db, vector_distance, trigram_similarity = row

            # Normaliza distância
            sim_vector = 1 / (1 + vector_distance) if vector_distance is not None else 0
            trigram_sim = trigram_similarity if trigram_similarity else 0

            # Interseção de tokens
            palavras_encontradas = []
            if tokens_db:
                found = set(tokens_db).intersection(set_query_tokens)
                palavras_encontradas = list(found)

            qtd_encontradas = len(palavras_encontradas)
            ratio_tokens = qtd_encontradas / len(query_tokens) if len(query_tokens) > 0 else 0

            # Combina vetorial + trigram + tokens
            final_score = (sim_vector + trigram_sim + ratio_tokens) / 3

            resultados_com_score.append((
                id,
                combined_text,
                vector_distance,
                trigram_sim,
                final_score,
                palavras_encontradas,
                qtd_encontradas
            ))

        df_result = pd.DataFrame(
            resultados_com_score,
            columns=[
                'id',
                'combined_text',
                'vector_distance',
                'trigram_similarity',
                'final_score',
                'palavras_encontradas',
                'qtd_encontradas'
            ]
        )

        df_result.sort_values('final_score', ascending=False, inplace=True)

        return df_result.head(top_k)

    except Exception as e:
        print(f"Erro na transação: {e}")
        connV.rollback()  # Desfaz a transação atual
        raise  # Relevanta a exceção para tratamento posterior


# In[116]:


consulta = '''
quais trabalhos tem relação com plr
'''


# In[141]:


select_query = """
    SELECT
        nr_trabalho,
        combined_text,
        tokens,
        embedding_vector <-> {embedding_query_str}::vector AS vector_distance,
        similarity(combined_text, %s) AS trigram_similarity
    FROM ptai.com_resultado_tx_completo
    ORDER BY embedding_vector <-> {embedding_query_str}::vector
    LIMIT {limit}
"""

df_result = search_similar_texts(consulta, connV, top_k=50, select_query=select_query)


# In[142]:


pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_colwidth', None)


# In[143]:


df_result[['id', 'vector_distance', 'trigram_similarity', 'final_score', 'palavras_encontradas', 'qtd_encontradas']]
