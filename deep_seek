ALTER TABLE ptai.com_resultado_tx_completo
  ADD COLUMN tokens text[];

def tokenize_text_pt(text):
    import unicodedata, re
    text = text.lower()
    # Remove acentos
    text = unicodedata.normalize('NFD', text).encode('ascii','ignore').decode('utf-8')
    # Remove pontuação
    text = re.sub(r'[^\w\s]', '', text)
    # Split
    tokens = text.split()
    return tokens

def update_embeddings(df, connV):
    """
    Exemplo: atualiza embedding_vector E tokens
    """
    with connV.cursor() as curV:
        for index, row in df.iterrows():
            nr_trabalho = row['nr_trabalho']
            combined_text = row['combined_text']

            # 1) Gera embedding do texto
            doc = nlp(combined_text)
            embedding_spacy = doc.vector.tolist()

            # 2) Gera tokens
            tokens = tokenize_text_pt(combined_text)

            # 3) Faz UPDATE com embedding e tokens
            curV.execute("""
                UPDATE ptai.com_resultado_tx_completo
                SET embedding_vector = %s,
                    tokens = %s
                WHERE nr_trabalho = %s
            """, (embedding_spacy, tokens, nr_trabalho))

    connV.commit()


def search_similar_texts(query, connV, top_k=5):
    """
    Busca semântica + trigram, e também retorna:
    - palavras_encontradas (array)
    - qtd_encontradas (int)
    num DataFrame final.
    """
    # 1) Tokeniza a query do usuário
    query_tokens = tokenize_text_pt(query)

    # 2) Gera embedding da query (para parte vetorial)
    doc_query = nlp(query)
    embedding_query = doc_query.vector.tolist()
    embedding_query_str = f"ARRAY{embedding_query}"

    # 3) Puxa do banco: nr_trabalho, combined_text, tokens, distance, trigram
    with connV.cursor() as curV:
        curV.execute(f"""
            SELECT
                nr_trabalho,
                combined_text,
                tokens,
                embedding_vector <-> {embedding_query_str}::vector AS vector_distance,
                similarity(combined_text, %s) AS trigram_similarity
            FROM ptai.com_resultado_tx_completo
            ORDER BY embedding_vector <-> {embedding_query_str}::vector
            LIMIT {top_k * 5}
        """, (query,))

        rows = curV.fetchall()

    # rows = [(nr_trab, text, tokens, dist, trigram_sim), ...]

    # 4) Monta lista + DataFrame
    resultados_com_score = []
    for row in rows:
        nr_trabalho, combined_text, tokens_db, vector_distance, trigram_similarity = row

        # Normaliza a distância => similaridade vetorial ~ [0..1]
        if vector_distance is not None:
            sim_vector = 1 / (1 + vector_distance)
        else:
            sim_vector = 0

        # Combina o valor de trigram (só se não for None)
        trigram_sim = trigram_similarity if trigram_similarity else 0

        # Score final (exemplo: média)
        final_score = (sim_vector + trigram_sim) / 2

        # 5) Interseção: quais tokens do query estão em tokens_db?
        #    tokens_db é list ou None. Verifique:
        palavras_encontradas = []
        if tokens_db:
            set_tokens_db = set(tokens_db)
            set_query = set(query_tokens)
            # Interseção
            found = set_tokens_db.intersection(set_query)
            palavras_encontradas = list(found)
        
        qtd_encontradas = len(palavras_encontradas)

        resultados_com_score.append((
            nr_trabalho,
            combined_text,
            vector_distance,
            trigram_sim,
            final_score,
            palavras_encontradas,
            qtd_encontradas
        ))

    # 6) Converter em DataFrame
    df_result = pd.DataFrame(
        resultados_com_score,
        columns=[
            'nr_trabalho',
            'combined_text',
            'vector_distance',
            'trigram_similarity',
            'final_score',
            'palavras_encontradas',
            'qtd_encontradas'
        ]
    )

    # 7) Ordenar desc por final_score
    df_result.sort_values('final_score', ascending=False, inplace=True)

    # 8) Retornar top_k
    return df_result.head(top_k)





import unicodedata
import re

def normalize_text(text):
    # Converte para minúsculas
    text = text.lower()

    # Remove acentos (unicode -> ASCII)
    text = unicodedata.normalize('NFD', text)
    text = text.encode('ascii', 'ignore').decode('utf-8')

    # Remove pontuação e caracteres especiais (opcional)
    text = re.sub(r'[^\w\s]', '', text)

    # Remove espaços extras
    text = text.strip()

    return text


def update_embeddings(df, connV):
    try:
        with connV.cursor() as curV:
            for index, row in df.iterrows():
                nr_trabalho = row['nr_trabalho']
                combined_text = row['combined_text']

                # Normaliza o texto
                combined_text_norm = normalize_text(combined_text)

                # Gera embedding com spaCy (texto normalizado)
                doc = nlp(combined_text_norm)
                embedding_spacy = doc.vector.tolist()

                # Atualiza a coluna embedding_vector na tabela
                curV.execute("""
                    UPDATE ptai.com_resultado_tx_completo
                    SET embedding_vector = %s
                    WHERE nr_trabalho = %s
                """, (embedding_spacy, nr_trabalho))

        connV.commit()
    except Exception as e:
        print(f"Erro ao atualizar embeddings: {e}")
        connV.rollback()


def search_similar_texts(query, connV, top_k=5):
    # Normaliza a query
    query_norm = normalize_text(query)

    doc_query = nlp(query_norm)
    embedding_query = doc_query.vector.tolist()

    embedding_query_str = f"ARRAY{embedding_query}"

    with connV.cursor() as curV:
        curV.execute(f"""
            SELECT
                nr_trabalho,
                combined_text,
                embedding_vector <-> {embedding_query_str}::vector AS vector_distance,
                similarity(combined_text, %s) AS trigram_similarity
            FROM ptai.com_resultado_tx_completo
            ORDER BY embedding_vector <-> {embedding_query_str}::vector
            LIMIT {top_k * 5}
        """, (query_norm,))

        resultados = curV.fetchall()

    # Resto do código (normalização da distância, score etc.)...
    # ...



