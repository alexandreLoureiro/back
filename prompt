@router.post("/apiptai/chat")
def chat_endpoint(req: ChatRequest):
    """
    Endpoint para receber a mensagem do usuário e devolver a resposta da IA.
    """
    session_id = req.sessionId
    user_message = req.userMessage.strip()  # Remove espaços em branco
    role_user = req.roleUser
    role_assistant = req.roleAssistant

    # Verificar se os campos obrigatórios estão presentes
    if not session_id:
        return {"error": "Session ID is required"}
    if user_message == "" and (not role_user or not role_assistant):
        return {"error": "User message or initial context (roleUser and roleAssistant) is required"}

    # Inicializa a sessão se não existir
    if session_id not in sessions:
        sessions[session_id] = []
        print("Sessão inicializada")

    # Adiciona o contexto inicial à sessão, se fornecido
    if role_user and role_assistant and len(sessions[session_id]) == 0:
        print("Adicionando contexto inicial à sessão")
        sessions[session_id].append({"role": "assistant", "content": role_assistant})
        sessions[session_id].append({"role": "user", "content": role_user})

    # Adiciona a mensagem do usuário ao histórico, se for válida
    if user_message:
        sessions[session_id].append({"role": "user", "content": user_message})

    # Valida o histórico da sessão antes de enviar
    if len(sessions[session_id]) < 1:
        return {"error": "Session history is empty or invalid"}

    # Concatena o histórico da sessão para enviar ao modelo LLM
    prompt = "\n".join([f"{msg['role']}: {msg['content']}" for msg in sessions[session_id]])

    # Faz a chamada para o modelo usando a API LLM da empresa
    try:
        answer_text = assistentLLM(prompt, servico="URL_DA_API_LLM")
    except requests.exceptions.HTTPError as http_err:
        # Tratar erro HTTP (como 500)
        print(f"Erro HTTP na chamada ao serviço LLM: {http_err}")
        return {"error": "Erro interno no servidor. Por favor, tente novamente mais tarde."}
    except Exception as e:
        # Tratar outros erros
        print(f"Erro ao processar resposta da LLM: {e}")
        return {"error": "Erro ao acessar o serviço de IA. Por favor, tente novamente mais tarde."}

    # Armazena a resposta do assistente no histórico
    sessions[session_id].append({"role": "assistant", "content": answer_text})

    # Retorna a resposta em JSON
    return {"answer": answer_text}


def assistentLLM(prompt: str, servico: str) -> str:
    """
    Faz uma chamada ao serviço LLM da empresa.

    Args:
        prompt (str): Entrada do usuário.
        servico (str): URL do serviço LLM.

    Returns:
        str: Resposta gerada pelo modelo LLM.
    """
    try:
        # Configuração do payload
        payload = {
            "data": {
                "input": prompt,
                "intents": [{}],
                "entities": [{}],
                "context": {
                    "config": {
                        "top_p": 0.95,
                        "max_tokens": 4000,
                        "temperature": 0.2
                    }
                }
            }
        }

        # Chamada à API do serviço
        response = requests.post(
            servico,
            data=json.dumps(payload),
            headers={'Content-type': 'application/json', 'accept': 'application/json'}
        )
        response.raise_for_status()  # Levanta exceção para erros HTTP (4xx, 5xx)

        # Parse da resposta
        resp_dict = response.json()

        # Extração do status
        status = resp_dict.get('status')
        if status != "success":
            print(f"Erro na resposta da LLM: {status}")
            raise ValueError("Erro na geração da resposta pelo modelo.")

        # Extração do texto de resposta
        texto_resposta = resp_dict["data"]["context"]["prompt_response"]["acs_llm_prompt_execution"]["result"]
        return texto_resposta

    except requests.exceptions.HTTPError as http_err:
        print(f"Erro HTTP na chamada ao serviço LLM: {http_err}")
        raise  # Relança o erro para ser tratado no endpoint
    except Exception as e:
        print(f"Erro na chamada ao serviço LLM: {e}")
        raise  # Relança o erro para ser tratado no endpoint

