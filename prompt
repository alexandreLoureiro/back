@router.post("/apiptai/chat")
def chat_endpoint(req: ChatRequest):
    """
    Endpoint para receber a mensagem do usuário e devolver a resposta da IA.
    """
    session_id = req.sessionId
    user_message = req.userMessage.strip()  # Remove espaços em branco
    role_user = req.roleUser
    role_assistant = req.roleAssistant

    # Verificar se os campos obrigatórios estão presentes
    if not session_id:
        return {"error": "Session ID is required"}
    if user_message == "" and (not role_user or not role_assistant):
        return {"error": "User message or initial context (roleUser and roleAssistant) is required"}

    # Inicializa a sessão se não existir
    if session_id not in sessions:
        sessions[session_id] = []
        print("Sessão inicializada")

    # Adiciona o contexto inicial à sessão, se fornecido
    if role_user and role_assistant and len(sessions[session_id]) == 0:
        print("Adicionando contexto inicial à sessão")
        sessions[session_id].append({"role": "assistant", "content": role_assistant})
        sessions[session_id].append({"role": "user", "content": role_user})

    # Adiciona a mensagem do usuário ao histórico, se for válida
    if user_message:
        sessions[session_id].append({"role": "user", "content": user_message})

    # Valida o histórico da sessão antes de enviar
    if len(sessions[session_id]) < 1:
        return {"error": "Session history is empty or invalid"}

    # Concatena o histórico da sessão para enviar ao modelo LLM
    prompt = "\n".join([f"{msg['role']}: {msg['content']}" for msg in sessions[session_id]])

    # Faz a chamada para o modelo usando a API LLM da empresa
    try:
        answer_text = assistentLLM(prompt, servico="URL_DA_API_LLM")
    except requests.exceptions.HTTPError as http_err:
        # Tratar erro HTTP (como 500)
        print(f"Erro HTTP na chamada ao serviço LLM: {http_err}")
        return {"error": "Erro interno no servidor. Por favor, tente novamente mais tarde."}
    except Exception as e:
        # Tratar outros erros
        print(f"Erro ao processar resposta da LLM: {e}")
        return {"error": "Erro ao acessar o serviço de IA. Por favor, tente novamente mais tarde."}

    # Armazena a resposta do assistente no histórico
    sessions[session_id].append({"role": "assistant", "content": answer_text})

    # Retorna a resposta em JSON
    return {"answer": answer_text}
